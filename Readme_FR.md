<p align="center">
  <img src="https://user-images.githubusercontent.com/38581401/240117836-f06199ba-c80d-413a-9cb4-5adc76316bda.png" height="230" alt="MOSEC" />
</p>

<p align="center">
  <a href="https://discord.gg/Jq5vxuH69W">
    <img alt="lien d'invitation Discord" src="https://dcbadge.vercel.app/api/server/Jq5vxuH69W?style=flat">
  </a>
  <a href="https://pypi.org/project/mosec/">
    <img src="https://badge.fury.io/py/mosec.svg" alt="Version PyPI" height="20">
  </a>
  <a href="https://anaconda.org/conda-forge/mosec">
    <img src="https://anaconda.org/conda-forge/mosec/badges/version.svg" alt="conda-forge">
  </a>
  <a href="https://pypi.org/project/mosec">
    <img src="https://img.shields.io/pypi/pyversions/mosec" alt="Version Python" />
  </a>
  <a href="https://pepy.tech/project/mosec">
    <img src="https://static.pepy.tech/badge/mosec/month" alt="T√©l√©chargements mensuels PyPi" height="20">
  </a>
  <a href="https://tldrlegal.com/license/apache-license-2.0-(apache-2.0)">
    <img src="https://img.shields.io/github/license/mosecorg/mosec" alt="Licence" height="20">
  </a>
  <a href="https://github.com/mosecorg/mosec/actions/workflows/check.yml?query=workflow%3A%22lint+and+test%22+branch%3Amain">
    <img src="https://github.com/mosecorg/mosec/actions/workflows/check.yml/badge.svg?branch=main" alt="Statut des v√©rifications" height="20">
  </a>
</p>

<p align="center">
  <i>Service de mod√®les efficace dans le Cloud.</i>
</p>

## Introduction

<p align="center">
  <img src="https://user-images.githubusercontent.com/38581401/234162688-efd74e46-4063-4624-ac32-b197e4d8e56b.png" height="230" alt="MOSEC" />
</p>

Mosec est un cadre de d√©ploiement de mod√®les performant et flexible pour construire des microservices et des backends habilit√©s par des mod√®les d'apprentissage automatique. Il comble l'√©cart entre les mod√®les que vous avez form√©s et un service API en ligne efficace.

- **Tr√®s performant** : couche web et coordination des t√¢ches en Rust ü¶Ä, offrant une grande rapidit√© avec une utilisation efficace du CPU gr√¢ce √† l'I/O asynchrone.
- **Facilit√© d'utilisation** : interface utilisateur en Python üêç, permettant de servir les mod√®les de mani√®re ind√©pendante de tout cadre ML, avec le m√™me code que pour les tests hors ligne.
- **Regroupement dynamique** : agr√©ger des demandes d'utilisateurs diff√©rents pour une inf√©rence group√©e et redistribuer les r√©sultats.
- **√âtages en pipeline** : plusieurs processus g√®rent des charges de travail mixtes CPU/GPU/IO dans des √©tapes en pipeline.
- **Compatible avec le cloud** : con√ßu pour fonctionner dans le cloud, avec pr√©chauffage de mod√®le, arr√™t gracieux, et m√©triques de surveillance Prometheus, facilement g√©r√© par Kubernetes ou tout syst√®me d'orchestration de conteneurs.
- **Faire une chose bien** : se concentrer sur le d√©ploiement en ligne, pour que les utilisateurs puissent optimiser leur mod√®le et la logique m√©tier.

## Installation

Mosec n√©cessite Python 3.7 ou sup√©rieur. Installez la derni√®re version du [package PyPI](https://pypi.org/project/mosec/) pour Linux x86_64 ou macOS x86_64/ARM64 avec :

```shell
pip install -U mosec
# ou installez avec conda
conda install conda-forge::mosec

```
Pour construire √† partir du code source, installez [Rust](https://www.rust-lang.org/) et ex√©cutez la commande suivante :

```shell
make package
```

Vous obtiendrez un fichier wheel Mosec dans le dossier dist.

Utilisation
Nous d√©montrons comment Mosec peut vous aider √† h√©berger facilement un mod√®le stable de diffusion pr√©-entra√Æn√© en tant que service. Vous devez installer diffusers et transformers comme pr√©requis :
```
pip install --upgrade diffusers[torch] transformers
```
√âcrire le serveur
<details> <summary>Cliquez ici pour voir le code du serveur avec des explications.</summary>
Tout d'abord, nous importons les biblioth√®ques et configurons un journal de base pour mieux observer ce qui se passe.

```
from io import BytesIO
from typing import List

import torch  # type: ignore
from diffusers import StableDiffusionPipeline  # type: ignore

from mosec import Server, Worker, get_logger
from mosec.mixin import MsgpackMixin

logger = get_logger()
```

Ensuite, nous **construisons une API** pour que les clients puissent interroger une invite textuelle et obtenir une image bas√©e sur le mod√®le [stable-diffusion-v1-5](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5) en seulement 3 √©tapes.

1) D√©finissez votre service comme une classe qui h√©rite de `mosec.Worker`. Ici, nous h√©ritons √©galement de `MsgpackMixin` pour utiliser le format de s√©rialisation [msgpack](https://msgpack.org/index.html)<sup>(a)</sup>.

2) Dans la m√©thode `__init__`, initialisez votre mod√®le et placez-le sur l'appareil correspondant. Vous pouvez √©ventuellement attribuer `self.example` avec des donn√©es pour r√©chauffer<sup>(b)</sup> le mod√®le. Notez que les donn√©es doivent √™tre compatibles avec le format d'entr√©e de votre gestionnaire, que nous d√©taillons ci-dessous.

3) Remplacez la m√©thode `forward` pour √©crire votre gestionnaire de service<sup>(c)</sup>, avec la signature `forward(self, data: Any | List[Any]) -> Any | List[Any]`. La r√©ception ou le retour d'un seul √©l√©ment ou d'un tuple d√©pend de la configuration ou non du [batching dynamique](#configuration)<sup>(d)</sup>.
```python
class StableDiffusion(MsgpackMixin, Worker):
    def __init__(self):
        self.pipe = StableDiffusionPipeline.from_pretrained(
            "sd-legacy/stable-diffusion-v1-5", torch_dtype=torch.float16
        )
        self.pipe.enable_model_cpu_offload()
        self.example = ["useless example prompt"] * 4  # warmup (batch_size=4)

    def forward(self, data: List[str]) -> List[memoryview]:
        logger.debug("generate images for %s", data)
        res = self.pipe(data)
        logger.debug("NSFW: %s", res[1])
        images = []
        for img in res[0]:
            dummy_file = BytesIO()
            img.save(dummy_file, format="JPEG")
            images.append(dummy_file.getbuffer())
        return images
```
> [!NOTE]
>
> (a) Dans cet exemple, nous retournons une image au format binaire, que JSON ne prend pas en charge (sauf s'il est encod√© en base64, ce qui augmente la taille de la charge utile). Par cons√©quent, msgpack convient mieux √† nos besoins. Si nous n'h√©ritons pas de `MsgpackMixin`, JSON sera utilis√© par d√©faut. En d'autres termes, le protocole de la requ√™te/r√©ponse du service peut √™tre msgpack, JSON ou tout autre format (consultez nos [mixins](https://mosecorg.github.io/mosec/reference/interface.html#module-mosec.mixin)).
>
> (b) Le pr√©chauffage aide g√©n√©ralement √† allouer la m√©moire GPU √† l'avance. Si l'exemple de pr√©chauffage est sp√©cifi√©, le service ne sera pr√™t qu'apr√®s que l'exemple ait √©t√© transmis au gestionnaire. Cependant, si aucun exemple n'est donn√©, la latence de la premi√®re requ√™te sera probablement plus longue. `example` doit √™tre d√©fini comme un √©l√©ment unique ou un tuple en fonction de ce que `forward` attend de recevoir. De plus, dans le cas o√π vous souhaitez effectuer un pr√©chauffage avec plusieurs exemples diff√©rents, vous pouvez d√©finir `multi_examples` (d√©monstration [ici](https://mosecorg.github.io/mosec/examples/jax.html)).
>
> (c) Cet exemple montre un service √† un seul √©tage, o√π le worker `StableDiffusion` prend directement la requ√™te de l'utilisateur et r√©pond avec une image. Ainsi, `forward` peut √™tre consid√©r√© comme le gestionnaire de service complet. Cependant, nous pouvons √©galement concevoir un service √† plusieurs √©tages avec des workers effectuant diff√©rentes t√¢ches (par exemple, t√©l√©chargement d'images, inf√©rence du mod√®le, post-traitement) dans un pipeline. Dans ce cas, tout le pipeline est consid√©r√© comme le gestionnaire de service, avec le premier worker recevant la requ√™te et le dernier envoyant la r√©ponse. Le flux de donn√©es entre les workers est r√©alis√© par communication inter-processus.
>
> (d) √âtant donn√© que le regroupement dynamique est activ√© dans cet exemple, la m√©thode `forward` recevra id√©alement une _liste_ de cha√Ænes de caract√®res, par exemple, `['un chat mignon jouant avec une balle rouge', 'un homme assis devant un ordinateur', ...]`, agr√©g√©es √† partir de diff√©rents clients pour une _inf√©rence group√©e_, am√©liorant ainsi le d√©bit du syst√®me.

```python
if __name__ == "__main__":
    server = Server()
    # 1) `num` sp√©cifie le nombre de processus qui seront lanc√©s en parall√®le.
    # 2) En configurant `max_batch_size` avec une valeur > 1, les donn√©es d'entr√©e dans votre
    # fonction `forward` seront une liste (batch); sinon, c'est un √©l√©ment unique.
    server.append_worker(StableDiffusion, num=1, max_batch_size=4, max_wait_time=10)
    server.run()
```
</details>

### Ex√©cuter le serveur

<details>
<summary>Cliquez ici pour voir comment ex√©cuter et interroger le serveur.</summary>

Les extraits ci-dessus sont fusionn√©s dans notre fichier d'exemple. Vous pouvez directement l'ex√©cuter √† la racine du projet. Jetons d'abord un coup d'≈ìil aux _arguments en ligne de commande_ (explications [ici](https://mosecorg.github.io/mosec/reference/arguments.html)) :

```shell
python examples/stable_diffusion/server.py --help
```
Ensuite, d√©marrons le serveur avec des journaux de d√©bogage :

```
python examples/stable_diffusion/server.py --log-level debug --timeout 30000
```

Ouvrez http://127.0.0.1:8000/openapi/swagger/ dans votre navigateur pour obtenir la documentation OpenAPI.

Et dans un autre terminal, testez-le :

```
python examples/stable_diffusion/client.py --prompt "un mignon chat jouant avec une balle rouge" --output chat.jpg --port 8000
```
Vous obtiendrez une image nomm√©e "chat.jpg" dans le r√©pertoire courant.

Vous pouvez v√©rifier les m√©triques :
```
curl http://127.0.0.1:8000/metrics
```
Et voil√† ! Vous venez d'h√©berger votre mod√®le stable-diffusion en tant que service ! üòâ

</details> ```

## Exemples

Vous pouvez trouver plus d'exemples pr√™ts √† l'emploi dans la section [Exemple](https://mosecorg.github.io/mosec/examples/index.html). Elle inclut :

- [Pipeline](https://mosecorg.github.io/mosec/examples/echo.html) : une simple d√©mo echo sans aucun mod√®le ML.
- [Validation des requ√™tes](https://mosecorg.github.io/mosec/examples/validate.html) : validez la requ√™te avec une annotation de type.
- [Route multiple](https://mosecorg.github.io/mosec/examples/multi_route.html) : h√©bergez plusieurs mod√®les dans un seul service.
- [Service d'embedding](https://mosecorg.github.io/mosec/examples/embedding.html) : service d'embedding compatible avec OpenAI.
- [Service de reranking](https://mosecorg.github.io/mosec/examples/rerank.html) : r√©organisez une liste de passages en fonction d'une requ√™te.
- [IPC via m√©moire partag√©e](https://mosecorg.github.io/mosec/examples/ipc.html) : communication inter-processus avec m√©moire partag√©e.
- [Allocation de GPU personnalis√©e](https://mosecorg.github.io/mosec/examples/env.html) : d√©ployez plusieurs r√©pliques, chacune utilisant des GPU diff√©rents.
- [M√©triques personnalis√©es](https://mosecorg.github.io/mosec/examples/metric.html) : enregistrez vos propres m√©triques pour la surveillance.
- [Inference Jax jitt√©e](https://mosecorg.github.io/mosec/examples/jax.html) : la compilation just-in-time acc√©l√®re l'inf√©rence.
- Mod√®les de deep learning avec PyTorch :
  - [Analyse de sentiments](https://mosecorg.github.io/mosec/examples/pytorch.html#natural-language-processing) : inf√©rer le sentiment d'une phrase.
  - [Reconnaissance d'image](https://mosecorg.github.io/mosec/examples/pytorch.html#computer-vision) : cat√©goriser une image donn√©e.
  - [Stable diffusion](https://mosecorg.github.io/mosec/examples/stable_diffusion.html) : g√©n√©rer des images √† partir de textes, avec la s√©rialisation msgpack.


## Configuration

- **Batching dynamique**
  - Configurez `max_batch_size` et `max_wait_time` (en millisecondes) lorsque vous appelez `append_worker`.
  - Assurez-vous que l'inf√©rence avec `max_batch_size` ne provoque pas de probl√®mes de m√©moire sur le GPU.
  - En g√©n√©ral, `max_wait_time` doit √™tre inf√©rieur au temps d'inf√©rence du batch.
  - Si activ√©, le service collecte un lot lorsque soit le nombre de requ√™tes accumul√©es atteint `max_batch_size`, soit lorsque `max_wait_time` est √©coul√©. Cela est b√©n√©fique lorsque le trafic est √©lev√©.

Consultez la [documentation des arguments](https://mosecorg.github.io/mosec/reference/arguments.html) pour d'autres configurations.

## D√©ploiement

- Pour une image de base GPU avec `mosec` install√©, consultez l'image officielle [`mosecorg/mosec`](https://hub.docker.com/r/mosecorg/mosec). Pour des cas d'utilisation plus complexes, envisagez d'utiliser [envd](https://github.com/tensorchord/envd).
- Ce service n'a pas besoin de Gunicorn ou NGINX, mais vous pouvez utiliser un contr√¥leur d'entr√©e si n√©cessaire.
- Le service doit √™tre le processus PID 1 dans le conteneur car il g√®re plusieurs processus. Si vous devez ex√©cuter plusieurs processus dans un conteneur, utilisez un superviseur comme [Supervisor](https://github.com/Supervisor/supervisor) ou [Horust](https://github.com/FedericoPonzi/Horust).
- **M√©triques** √† collecter :
  - `mosec_service_batch_size_bucket` : montre la distribution des tailles de batch.
  - `mosec_service_batch_duration_second_bucket` : montre la dur√©e du batching dynamique pour chaque connexion √† chaque √©tape (√† partir de la r√©ception de la premi√®re t√¢che).
  - `mosec_service_process_duration_second_bucket` : montre la dur√©e de traitement pour chaque connexion √† chaque √©tape (y compris le temps IPC, mais excluant la `mosec_service_batch_duration_second_bucket`).
  - `mosec_service_remaining_task` : montre le nombre de t√¢ches en cours de traitement.
  - `mosec_service_throughput` : montre le d√©bit du service.

## Optimisation des performances

- Trouvez les meilleures valeurs pour `max_batch_size` et `max_wait_time` pour votre service d'inf√©rence. Les m√©triques montreront les histogrammes de la taille r√©elle des lots et de leur dur√©e. Ces informations sont essentielles pour ajuster ces deux param√®tres.
- Essayez de diviser tout le processus d'inf√©rence en √©tapes CPU et GPU s√©par√©es (r√©f√©rence [DistilBERT](https://mosecorg.github.io/mosec/examples/pytorch.html#natural-language-processing)). Diff√©rentes √©tapes seront ex√©cut√©es dans un [pipeline de donn√©es](https://fr.wikipedia.org/wiki/Pipeline_(informatique)), ce qui maintiendra le GPU occup√©.
- Vous pouvez √©galement ajuster le nombre de workers pour chaque √©tape. Par exemple, si votre pipeline se compose d'une √©tape CPU pour le pr√©traitement et d'une √©tape GPU pour l'inf√©rence du mod√®le, augmenter le nombre de workers pour l'√©tape CPU peut aider √† produire plus de donn√©es √† traiter en batch pour l'√©tape d'inf√©rence GPU. L'augmentation des workers de l'√©tape GPU peut maximiser l'utilisation de la m√©moire et de la puissance de calcul du GPU. Les deux m√©thodes peuvent contribuer √† une utilisation plus efficace du GPU, entra√Ænant ainsi un d√©bit de service plus √©lev√©.
- Pour les services multi-√©tapes, notez que les donn√©es passant entre les diff√©rentes √©tapes seront s√©rialis√©es/d√©s√©rialis√©es via les m√©thodes `serialize_ipc/deserialize_ipc`, donc des donn√©es extr√™mement volumineuses pourraient ralentir tout le pipeline. Les donn√©es s√©rialis√©es sont transmises √† l'√©tape suivante via Rust par d√©faut, mais vous pouvez activer la m√©moire partag√©e pour r√©duire potentiellement la latence (r√©f√©rence [RedisShmIPCMixin](https://mosecorg.github.io/mosec/examples/ipc.html#redis-shm-ipc-py)).
- Choisissez des m√©thodes appropri√©es pour la s√©rialisation/d√©s√©rialisation, utilis√©es pour d√©coder les requ√™tes utilisateur et encoder les r√©ponses. Par d√©faut, les deux utilisent JSON. Cependant, les images et les embeddings ne sont pas bien pris en charge par JSON. Vous pouvez choisir msgpack, plus rapide et compatible avec les donn√©es binaires (r√©f√©rence [Stable Diffusion](https://mosecorg.github.io/mosec/examples/stable_diffusion.html)).
- Configurez les threads pour OpenBLAS ou MKL. Il se peut qu'ils ne choisissent pas les CPU les plus adapt√©s pour le processus Python en cours. Vous pouvez les configurer pour chaque worker en utilisant [env](https://mosecorg.github.io/mosec/reference/interface.html#mosec.server.Server.append_worker) (r√©f√©rence [allocation GPU personnalis√©e](https://mosecorg.github.io/mosec/examples/env.html)).
- Activez HTTP/2 c√¥t√© client. `mosec` s'adapte automatiquement au protocole de l'utilisateur (par exemple, HTTP/2) depuis la version v0.8.8.


## Utilisateurs

Voici quelques entreprises et utilisateurs individuels qui utilisent Mosec :

- [Modelz](https://modelz.ai) : Plateforme serverless pour l'inf√©rence ML.
- [MOSS](https://github.com/OpenLMLab/MOSS/blob/main/README_en.md) : Un mod√®le conversationnel open source similaire √† ChatGPT.
- [TencentCloud](https://www.tencentcloud.com/document/product/1141/45261) : Plateforme de machine learning de Tencent Cloud, utilisant Mosec comme [cadre principal du serveur d'inf√©rence](https://cloud.tencent.com/document/product/851/74148).
- [TensorChord](https://github.com/tensorchord) : Soci√©t√© d'infrastructure IA native du cloud.

## Citation

Si vous trouvez ce logiciel utile pour vos recherches, merci de bien vouloir le citer

```
@software{yang2021mosec,
  title = {{MOSEC: Model Serving made Efficient in the Cloud}},
  author = {Yang, Keming and Liu, Zichen and Cheng, Philip},
  url = {https://github.com/mosecorg/mosec},
  year = {2021}
}
```


## Contribuer

Nous acceptons toute forme de contribution. Vous pouvez nous donner votre avis en [cr√©ant des issues](https://github.com/mosecorg/mosec/issues/new/choose) ou en discutant sur [Discord](https://discord.gg/Jq5vxuH69W). Vous pouvez aussi directement [contribuer](https://mosecorg.github.io/mosec/development/contributing.html) en soumettant votre code et en faisant une pull request !

Pour commencer le d√©veloppement, vous pouvez utiliser [envd](https://github.com/tensorchord/envd) pour cr√©er un environnement isol√© et propre en Python & Rust. Consultez la [documentation envd](https://envd.tensorchord.ai/) ou le fichier [build.envd](https://github.com/mosecorg/mosec/blob/main/build.envd) pour plus d'informations.
